{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM5UhB17MK8EBTYUwyIcbDR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ttt888kkk/simplechat/blob/main/day2_practice_homework2_ipynb_%E3%81%AE%E3%82%B3%E3%83%94%E3%83%BC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IoEtKogEVCRz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a595d501-913b-4fcc-88ee-34778794071c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lecture-ai-engineering'...\n",
            "remote: Enumerating objects: 52, done.\u001b[K\n",
            "remote: Total 52 (delta 0), reused 0 (delta 0), pack-reused 52 (from 1)\u001b[K\n",
            "Receiving objects: 100% (52/52), 83.21 KiB | 5.55 MiB/s, done.\n",
            "Resolving deltas: 100% (9/9), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/matsuolab/lecture-ai-engineering.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ttt888kkk/simplechat.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZSHWEpCVkav",
        "outputId": "57b4ea71-ef90-4cb4-a0a3-a5bd793871f8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'simplechat'...\n",
            "remote: Enumerating objects: 123, done.\u001b[K\n",
            "remote: Counting objects: 100% (53/53), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 123 (delta 41), reused 35 (delta 35), pack-reused 70 (from 2)\u001b[K\n",
            "Receiving objects: 100% (123/123), 178.48 KiB | 1.08 MiB/s, done.\n",
            "Resolving deltas: 100% (55/55), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "%cd /content/lecture-ai-engineering/day1\n",
        "load_dotenv(find_dotenv())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grPW05omWAYN",
        "outputId": "7672e337-c44a-4255-fac9-747fa5090c61"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.0)\n",
            "/content/lecture-ai-engineering/day1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/lecture-ai-engineering/day1/03_FastAPI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuaJfOZ_WPrV",
        "outputId": "133cc500-f1c4-444d-9767-0e2155b45fed"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/lecture-ai-engineering/day1/03_FastAPI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "pbCvYWt4cNjK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken $$NGROK_TOKEN\n",
        "!huggingface-cli login --token $$HUGGINGFACE_TOKEN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TC848KwIcUCb",
        "outputId": "006a8ceb-2a7e-4a8b-a1dc-7e9eeffda4fc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "The token `Tatsuki Kondo` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `Tatsuki Kondo`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/lecture-ai-engineering/day1/03_FastAPI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vGl6BLlcwCj",
        "outputId": "c0d465b8-3690-4fd3-df37-aba4045aae19"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/lecture-ai-engineering/day1/03_FastAPI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python app.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paNz1JsMdTiR",
        "outputId": "7a98a106-efe4-47d7-ea02-311d76afb216"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-05-07 07:09:01.524691: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1746601741.804930   16546 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1746601741.881172   16546 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-07 07:09:02.469565: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "„É¢„Éá„É´Âêç„ÇíË®≠ÂÆö: google/gemma-2-2b-jpn-it\n",
            "/content/lecture-ai-engineering/day1/03_FastAPI/app.py:134: DeprecationWarning: \n",
            "        on_event is deprecated, use lifespan event handlers instead.\n",
            "\n",
            "        Read more about it in the\n",
            "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
            "        \n",
            "  @app.on_event(\"startup\")\n",
            "FastAPI„Ç®„É≥„Éâ„Éù„Ç§„É≥„Éà„ÇíÂÆöÁæ©„Åó„Åæ„Åó„Åü„ÄÇ\n",
            "„Ç¢„ÇØ„ÉÜ„Ç£„Éñ„Å™ngrok„Éà„É≥„Éç„É´„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ\n",
            "„Éù„Éº„Éà8501„Å´Êñ∞„Åó„ÅÑngrok„Éà„É≥„Éç„É´„ÇíÈñã„ÅÑ„Å¶„ÅÑ„Åæ„Åô...\n",
            "---------------------------------------------------------------------\n",
            "‚úÖ ÂÖ¨ÈñãURL:   https://7a79-34-91-163-217.ngrok-free.app\n",
            "üìñ API„Éâ„Ç≠„É•„É°„É≥„Éà (Swagger UI): https://7a79-34-91-163-217.ngrok-free.app/docs\n",
            "---------------------------------------------------------------------\n",
            "(API„ÇØ„É©„Ç§„Ç¢„É≥„Éà„ÇÑ„Éñ„É©„Ç¶„Ç∂„Åã„Çâ„Ç¢„ÇØ„Çª„Çπ„Åô„Çã„Åü„ÇÅ„Å´„Åì„ÅÆURL„Çí„Ç≥„Éî„Éº„Åó„Å¶„Åè„Å†„Åï„ÅÑ)\n",
            "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m16546\u001b[0m]\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
            "load_model_task: „É¢„Éá„É´„ÅÆË™≠„ÅøËæº„Åø„ÇíÈñãÂßã...\n",
            "‰ΩøÁî®„Éá„Éê„Ç§„Çπ: cuda\n",
            "config.json: 100% 805/805 [00:00<00:00, 4.55MB/s]\n",
            "model.safetensors.index.json: 100% 24.2k/24.2k [00:00<00:00, 22.9MB/s]\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00001-of-00002.safetensors:   0% 0.00/4.99G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:   0% 0.00/241M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:   4% 10.5M/241M [00:00<00:02, 99.9MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 21.0M/4.99G [00:00<00:43, 114MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  13% 31.5M/241M [00:00<00:01, 123MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 41.9M/4.99G [00:00<00:38, 129MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  22% 52.4M/241M [00:00<00:01, 152MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 62.9M/4.99G [00:00<00:34, 142MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  30% 73.4M/241M [00:00<00:01, 145MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 83.9M/4.99G [00:00<00:37, 132MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  39% 94.4M/241M [00:00<00:01, 126MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 105M/4.99G [00:00<00:36, 135MB/s] \u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  48% 115M/241M [00:00<00:00, 128MB/s] \u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 126M/4.99G [00:00<00:34, 142MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  57% 136M/241M [00:01<00:00, 130MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 147M/4.99G [00:01<00:35, 135MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  65% 157M/241M [00:01<00:00, 142MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 168M/4.99G [00:01<00:36, 131MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  74% 178M/241M [00:01<00:00, 144MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 189M/4.99G [00:01<00:36, 132MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  83% 199M/241M [00:01<00:00, 139MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 210M/4.99G [00:01<00:34, 138MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors:  91% 220M/241M [00:01<00:00, 150MB/s]\u001b[A\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 231M/4.99G [00:01<00:33, 143MB/s]\u001b[A\n",
            "\n",
            "model-00002-of-00002.safetensors: 100% 241M/241M [00:01<00:00, 141MB/s]\n",
            "\n",
            "model-00001-of-00002.safetensors:   5% 262M/4.99G [00:01<00:27, 172MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 283M/4.99G [00:01<00:30, 154MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 325M/4.99G [00:02<00:22, 204MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 367M/4.99G [00:02<00:18, 247MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 398M/4.99G [00:02<00:18, 245MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 430M/4.99G [00:02<00:24, 187MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 472M/4.99G [00:02<00:20, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 503M/4.99G [00:02<00:18, 239MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 535M/4.99G [00:03<00:20, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 577M/4.99G [00:03<00:17, 248MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 619M/4.99G [00:03<00:15, 280MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 661M/4.99G [00:03<00:14, 308MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 703M/4.99G [00:03<00:17, 243MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 744M/4.99G [00:03<00:15, 272MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 776M/4.99G [00:03<00:17, 244MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 807M/4.99G [00:04<00:21, 196MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 849M/4.99G [00:04<00:17, 232MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 891M/4.99G [00:04<00:15, 259MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 933M/4.99G [00:04<00:15, 263MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 965M/4.99G [00:04<00:16, 242MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 996M/4.99G [00:04<00:16, 239MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.03G/4.99G [00:04<00:16, 237MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.07G/4.99G [00:05<00:15, 256MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.10G/4.99G [00:05<00:16, 239MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.14G/4.99G [00:05<00:14, 265MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.17G/4.99G [00:05<00:15, 244MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.22G/4.99G [00:05<00:14, 265MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.25G/4.99G [00:05<00:14, 251MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.28G/4.99G [00:05<00:14, 248MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.31G/4.99G [00:06<00:15, 245MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.34G/4.99G [00:06<00:14, 249MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.37G/4.99G [00:06<00:15, 241MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.41G/4.99G [00:06<00:14, 247MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.44G/4.99G [00:06<00:15, 230MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.48G/4.99G [00:06<00:13, 256MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.52G/4.99G [00:06<00:11, 289MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.55G/4.99G [00:07<00:12, 265MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.58G/4.99G [00:07<00:13, 246MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.61G/4.99G [00:07<00:14, 235MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.65G/4.99G [00:07<00:14, 228MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.68G/4.99G [00:07<00:16, 202MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.71G/4.99G [00:10<01:38, 33.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.73G/4.99G [00:10<01:20, 40.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.75G/4.99G [00:10<01:05, 49.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.77G/4.99G [00:10<00:53, 59.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.79G/4.99G [00:11<00:44, 71.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.81G/4.99G [00:11<00:38, 83.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.85G/4.99G [00:11<00:28, 111MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.89G/4.99G [00:11<00:20, 153MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.92G/4.99G [00:11<00:17, 180MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.96G/4.99G [00:11<00:13, 222MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 1.99G/4.99G [00:11<00:12, 242MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.02G/4.99G [00:11<00:11, 254MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.06G/4.99G [00:11<00:11, 263MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.09G/4.99G [00:12<00:11, 247MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.12G/4.99G [00:12<00:13, 218MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.15G/4.99G [00:12<00:14, 194MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.18G/4.99G [00:12<00:13, 201MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.21G/4.99G [00:12<00:13, 208MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.24G/4.99G [00:12<00:14, 190MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.26G/4.99G [00:16<02:00, 22.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.31G/4.99G [00:16<01:15, 35.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.35G/4.99G [00:16<00:50, 52.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.39G/4.99G [00:17<00:35, 74.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.42G/4.99G [00:17<00:28, 91.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.45G/4.99G [00:17<00:22, 113MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.50G/4.99G [00:17<00:17, 145MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.53G/4.99G [00:17<00:14, 169MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.57G/4.99G [00:17<00:11, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.61G/4.99G [00:17<00:10, 221MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.64G/4.99G [00:17<00:10, 224MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.67G/4.99G [00:18<00:10, 217MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.71G/4.99G [00:18<00:10, 224MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.74G/4.99G [00:18<00:09, 229MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.77G/4.99G [00:18<00:11, 199MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.80G/4.99G [00:24<02:12, 16.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.84G/4.99G [00:24<01:26, 24.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.87G/4.99G [00:25<01:04, 33.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.89G/4.99G [00:25<00:52, 40.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.93G/4.99G [00:25<00:37, 54.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.96G/4.99G [00:25<00:27, 72.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.99G/4.99G [00:25<00:21, 93.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.02G/4.99G [00:25<00:16, 118MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.05G/4.99G [00:25<00:13, 141MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.08G/4.99G [00:25<00:11, 167MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.11G/4.99G [00:25<00:10, 178MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.15G/4.99G [00:26<00:09, 204MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.18G/4.99G [00:26<00:09, 194MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.21G/4.99G [00:26<00:08, 204MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.24G/4.99G [00:26<00:11, 154MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.26G/4.99G [00:30<01:26, 20.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.30G/4.99G [00:31<00:53, 31.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.36G/4.99G [00:31<00:31, 51.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.40G/4.99G [00:31<00:22, 70.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.43G/4.99G [00:31<00:18, 86.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.46G/4.99G [00:31<00:14, 106MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.50G/4.99G [00:31<00:10, 139MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.54G/4.99G [00:31<00:08, 175MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.59G/4.99G [00:31<00:07, 179MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.62G/4.99G [00:32<00:06, 197MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.65G/4.99G [00:32<00:07, 191MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.69G/4.99G [00:32<00:05, 226MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.72G/4.99G [00:32<00:05, 237MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.75G/4.99G [00:35<00:31, 39.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.77G/4.99G [00:35<00:25, 47.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.81G/4.99G [00:35<00:18, 63.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.84G/4.99G [00:35<00:15, 71.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.87G/4.99G [00:35<00:12, 92.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.90G/4.99G [00:36<00:22, 49.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.92G/4.99G [00:37<00:18, 58.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.95G/4.99G [00:37<00:13, 77.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.98G/4.99G [00:37<00:10, 97.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.02G/4.99G [00:37<00:08, 121MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.05G/4.99G [00:37<00:06, 147MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.08G/4.99G [00:37<00:05, 168MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.11G/4.99G [00:39<00:14, 59.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.14G/4.99G [00:39<00:10, 78.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.17G/4.99G [00:40<00:16, 48.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.19G/4.99G [00:41<00:22, 34.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.23G/4.99G [00:41<00:15, 47.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.26G/4.99G [00:41<00:11, 65.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.28G/4.99G [00:41<00:09, 77.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.32G/4.99G [00:42<00:05, 112MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.36G/4.99G [00:42<00:04, 148MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.40G/4.99G [00:42<00:03, 185MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.44G/4.99G [00:42<00:02, 189MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.47G/4.99G [00:42<00:02, 196MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.50G/4.99G [00:42<00:02, 209MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.53G/4.99G [00:42<00:02, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.56G/4.99G [00:43<00:01, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.60G/4.99G [00:43<00:01, 249MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.63G/4.99G [00:43<00:01, 254MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.67G/4.99G [00:43<00:01, 243MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.71G/4.99G [00:43<00:01, 273MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.74G/4.99G [00:43<00:00, 274MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.77G/4.99G [00:43<00:00, 224MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.81G/4.99G [00:43<00:00, 258MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.84G/4.99G [00:44<00:00, 243MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.88G/4.99G [00:44<00:00, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.92G/4.99G [00:44<00:00, 240MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 4.99G/4.99G [00:44<00:00, 112MB/s]\n",
            "Fetching 2 files: 100% 2/2 [00:44<00:00, 22.40s/it]\n",
            "Loading checkpoint shards: 100% 2/2 [00:00<00:00,  6.54it/s]\n",
            "generation_config.json: 100% 168/168 [00:00<00:00, 1.06MB/s]\n",
            "tokenizer_config.json: 100% 46.9k/46.9k [00:00<00:00, 106MB/s]\n",
            "tokenizer.model: 100% 4.24M/4.24M [00:00<00:00, 85.3MB/s]\n",
            "tokenizer.json: 100% 17.5M/17.5M [00:00<00:00, 259MB/s]\n",
            "special_tokens_map.json: 100% 555/555 [00:00<00:00, 4.85MB/s]\n",
            "Device set to use cuda\n",
            "„É¢„Éá„É´ 'google/gemma-2-2b-jpn-it' „ÅÆË™≠„ÅøËæº„Åø„Å´ÊàêÂäü„Åó„Åæ„Åó„Åü\n",
            "load_model_task: „É¢„Éá„É´„ÅÆË™≠„ÅøËæº„Åø„ÅåÂÆå‰∫Ü„Åó„Åæ„Åó„Åü„ÄÇ\n",
            "Ëµ∑ÂãïÊôÇ„Å´„É¢„Éá„É´„ÅÆÂàùÊúüÂåñ„ÅåÂÆå‰∫Ü„Åó„Åæ„Åó„Åü„ÄÇ\n",
            "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
            "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:8501\u001b[0m (Press CTRL+C to quit)\n",
            "\u001b[32mINFO\u001b[0m:     Shutting down\n",
            "\u001b[32mINFO\u001b[0m:     Finished server process [\u001b[36m16546\u001b[0m]\n",
            "\n",
            "„Çµ„Éº„Éê„Éº„Éó„É≠„Çª„Çπ„ÅåÁµÇ‰∫Ü„Åó„Åæ„Åó„Åü„ÄÇ\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-1' coro=<Server.serve() done, defined at /usr/local/lib/python3.11/dist-packages/uvicorn/server.py:68> exception=KeyboardInterrupt()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/main.py\", line 580, in run\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 66, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 360, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 69, in serve\n",
            "    with self.capture_signals():\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 330, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "KeyboardInterrupt\n",
            "\u001b[31mERROR\u001b[0m:    Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 699, in lifespan\n",
            "    await receive()\n",
            "GeneratorExit\n",
            "\n",
            "Task was destroyed but it is pending!\n",
            "task: <Task pending name='Task-2' coro=<LifespanOn.main() done, defined at /usr/local/lib/python3.11/dist-packages/uvicorn/lifespan/on.py:78> wait_for=<Future cancelled>>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "BFbwdQADdcFp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qwuUf2b9mOZi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}